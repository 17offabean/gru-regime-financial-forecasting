{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "321389f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check Python executable path (useful for debugging environment issues)\n",
    "import sys \n",
    "print(sys.executable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d1d2b6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core libraries for data manipulation and numerical operations\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Financial data download\n",
    "import yfinance as yf\n",
    "\n",
    "# Machine learning preprocessing and metrics\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_error, r2_score\n",
    "\n",
    "# Deep learning framework for GRU model\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import GRU, Dense\n",
    "import tensorflow as tf\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "881acd8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For reproducibility\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d333737",
   "metadata": {},
   "source": [
    "<a id=\"feature-engineering\"></a>\n",
    "*FEATURE ENGINEERING*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f702ae4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# DATA DOWNLOAD AND PREPARATION\n",
    "# ============================================================================\n",
    "# Download 35 years of daily data for NASDAQ-100 index (^NDX)\n",
    "df = yf.download(\"^NDX\", period=\"35y\", interval=\"1d\")\n",
    "df.attrs['ticker'] = \"^NDX\"  # Store ticker symbol for later reference\n",
    "\n",
    "# Fix MultiIndex columns if yfinance returns them (flatten to single level)\n",
    "df.columns = df.columns.get_level_values(0)\n",
    "\n",
    "# Remove duplicate columns if any exist\n",
    "df = df.loc[:, ~df.columns.duplicated()]\n",
    "\n",
    "# ============================================================================\n",
    "# TECHNICAL INDICATORS\n",
    "# ============================================================================\n",
    "\n",
    "# --- Moving Averages ---\n",
    "# Simple Moving Averages: used to identify trends\n",
    "# SMA_10: Short-term trend (10-day average)\n",
    "# SMA_50: Medium-term trend (50-day average)\n",
    "df['SMA_10'] = df['Close'].rolling(window=10).mean()\n",
    "df['SMA_50'] = df['Close'].rolling(window=50).mean()\n",
    "\n",
    "# --- Log Returns ---\n",
    "# Log returns are preferred over simple returns because:\n",
    "# 1. They are symmetric (log(1/r) = -log(r))\n",
    "# 2. They are approximately normally distributed for small changes\n",
    "# 3. They are additive over time periods\n",
    "df['LogReturn'] = np.log(df['Close'] / df['Close'].shift(1))\n",
    "\n",
    "# --- Average True Range (ATR) ---\n",
    "# ATR measures market volatility by calculating the average of true ranges over a period\n",
    "# True Range is the maximum of: (High-Low), |High-PrevClose|, |Low-PrevClose|\n",
    "atr_period = 14  # Standard 14-period ATR\n",
    "true_range_high_low = df['High'] - df['Low']  # Current period range\n",
    "true_range_high_close = (df['High'] - df['Close'].shift(1)).abs()  # Gap up from previous close\n",
    "true_range_low_close = (df['Low'] - df['Close'].shift(1)).abs()  # Gap down from previous close\n",
    "# Take maximum of all three to get true range\n",
    "true_range = pd.concat([true_range_high_low, true_range_high_close, true_range_low_close], axis=1).max(axis=1)\n",
    "# Normalize ATR by price to make it comparable across different price levels\n",
    "df['ATR_14'] = (true_range.rolling(window=atr_period, min_periods=1).mean() / df['Close']).astype(float)\n",
    "\n",
    "# --- Relative Strength Index (RSI) ---\n",
    "# RSI is a momentum oscillator that measures speed and magnitude of price changes\n",
    "# Values above 70 typically indicate overbought conditions, below 30 indicate oversold\n",
    "rsi_period = 14  # Standard 14-period RSI\n",
    "price_change = df['Close'].diff()  # Calculate price changes\n",
    "gains = price_change.clip(lower=0)  # Only positive changes (gains)\n",
    "losses = -price_change.clip(upper=0)  # Only negative changes (losses), made positive\n",
    "avg_gains = gains.rolling(window=rsi_period, min_periods=1).mean()  # Average gains over period\n",
    "avg_losses = losses.rolling(window=rsi_period, min_periods=1).mean()  # Average losses over period\n",
    "rs = avg_gains / avg_losses.replace(0, np.nan)  # Relative strength ratio\n",
    "# RSI formula: 100 - (100 / (1 + RS)), then normalized to 0-1 range for model input\n",
    "df['RSI_14'] = (100 - (100 / (1 + rs))) / 100.0\n",
    "\n",
    "# --- Volume Ratio ---\n",
    "# Volume ratio compares current volume to average volume, indicating unusual trading activity\n",
    "# High volume ratios often precede significant price movements\n",
    "volume_sma_period = 20  # 20-day average volume\n",
    "df['Vol_SMA_20'] = df['Volume'].rolling(window=volume_sma_period, min_periods=1).mean()\n",
    "# Clip to 0-3 range to handle extreme volume spikes (winsorization)\n",
    "df['Volume_Ratio'] = (df['Volume'] / df['Vol_SMA_20']).clip(lower=0, upper=3)\n",
    "\n",
    "# --- Hurst Exponent ---\n",
    "# Hurst exponent measures the long-term memory of a time series\n",
    "# H < 0.5: Mean-reverting (anti-persistent)\n",
    "# H = 0.5: Random walk (no memory)\n",
    "# H > 0.5: Trending (persistent)\n",
    "def calculate_hurst_exponent(ts, lags_range=range(2, 100)):\n",
    "    \"\"\"\n",
    "    Calculate Hurst exponent using R/S (Rescaled Range) analysis.\n",
    "    \n",
    "    Args:\n",
    "        ts: Time series data\n",
    "        lags_range: Range of lag values to test\n",
    "    \n",
    "    Returns:\n",
    "        Hurst exponent value (typically between 0 and 1)\n",
    "    \"\"\"\n",
    "    lags = np.array(lags_range)\n",
    "    # Calculate standard deviation of differences at each lag\n",
    "    tau = [np.std(np.subtract(ts[lag:], ts[:-lag])) for lag in lags]\n",
    "    # Fit power law: log(tau) = H * log(lags) + c\n",
    "    poly = np.polyfit(np.log(lags), np.log(tau), 1)\n",
    "    return poly[0] * 2.0  # Multiply by 2 to get Hurst exponent\n",
    "\n",
    "hurst_window = 250  # Use 250 trading days (~1 year) for Hurst calculation\n",
    "hurst_values = []\n",
    "# Calculate rolling Hurst exponent for each point in the dataset\n",
    "for i in range(len(df)):\n",
    "    if i < hurst_window:\n",
    "        hurst_values.append(np.nan)  # Not enough data for early periods\n",
    "    else:\n",
    "        # Calculate Hurst for the window ending at current point\n",
    "        hurst_values.append(calculate_hurst_exponent(df['Close'].values[i-hurst_window:i]))\n",
    "df['Hurst'] = hurst_values\n",
    "\n",
    "# ============================================================================\n",
    "# FINAL CLEANUP\n",
    "# ============================================================================\n",
    "# Remove rows with NaN values (created by rolling calculations and shifts)\n",
    "df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6d22ada",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define which columns to use as features for the model\n",
    "# Includes raw OHLCV data plus all calculated technical indicators\n",
    "feature_cols = ['Open', 'High', 'Low', 'Close', 'Volume',\n",
    "                'SMA_10', 'SMA_50', 'ATR_14', 'RSI_14', 'Volume_Ratio', 'Hurst']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd482949",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# DATA PREPROCESSING FUNCTION\n",
    "# ============================================================================\n",
    "def create_windows(features, targets, window_size):\n",
    "    \"\"\"\n",
    "    Create sliding windows for time series data.\n",
    "    \n",
    "    For GRU models, we need sequences of historical data to predict the next value.\n",
    "    This function creates overlapping windows where each window contains 'window_size'\n",
    "    consecutive feature vectors, and the target is the value immediately after.\n",
    "    \n",
    "    Args:\n",
    "        features: Array of feature vectors (shape: [n_samples, n_features])\n",
    "        targets: Array of target values (shape: [n_samples,])\n",
    "        window_size: Number of time steps to include in each window\n",
    "    \n",
    "    Returns:\n",
    "        X: Windowed features (shape: [n_windows, window_size, n_features])\n",
    "        y: Target values aligned with windows (shape: [n_windows,])\n",
    "    \n",
    "    Example:\n",
    "        If window_size=3, features=[f1, f2, f3, f4, f5], targets=[t1, t2, t3, t4, t5]\n",
    "        Returns: X=[[f1,f2,f3], [f2,f3,f4]], y=[t4, t5]\n",
    "    \"\"\"\n",
    "    X, y = [], []\n",
    "    for i in range(len(targets) - window_size):\n",
    "        # Each window contains 'window_size' consecutive feature vectors\n",
    "        X.append(features[i:i+window_size])\n",
    "        # Target is the value 'window_size' steps ahead\n",
    "        y.append(targets[i+window_size])\n",
    "    return np.array(X), np.array(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1537f628",
   "metadata": {},
   "source": [
    "<a id=\"gru-model-architecture\"></a>\n",
    "*GRU Model Architecture*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6bf36f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# MARKET REGIME DEFINITIONS\n",
    "# ============================================================================\n",
    "# Define different market periods to test model performance across various conditions\n",
    "# Each regime represents a distinct market environment (crashes, booms, etc.)\n",
    "regimes = {\n",
    "    # NASDAQ & S&P500 CRASH REGIMES (^NDX)\n",
    "    # \"Dotcom_Crash\": (\"2000-01-01\", \"2002-12-31\"),  # Commented out - can be enabled\n",
    "    \"Global_Financial_Crisis\": (\"2007-07-01\", \"2009-03-31\"),  # 2008 financial crisis\n",
    "    \"AI_Boom\": (\"2023-01-01\", \"2024-06-30\"),  # Recent AI-driven market surge\n",
    "}\n",
    "\n",
    "# ============================================================================\n",
    "# WALK-FORWARD ANALYSIS\n",
    "# ============================================================================\n",
    "# Container to store results for each regime and model configuration\n",
    "regime_results = []\n",
    "\n",
    "# Main walk-forward loop: train on data before each regime, test on the regime period\n",
    "# This simulates real-world trading where we only use past data to predict future\n",
    "for regime_name, (start_date, end_date) in regimes.items():\n",
    "    print(f\"\\n=== Training up to {start_date}, testing on {regime_name} ({start_date} → {end_date}) ===\")\n",
    "\n",
    "    # ========================================================================\n",
    "    # DATA SPLITTING\n",
    "    # ========================================================================\n",
    "    # Split data chronologically: train on all data before regime, test on regime period\n",
    "    # This prevents look-ahead bias by ensuring test data comes after training data\n",
    "    train_df = df.loc[:pd.to_datetime(start_date) - pd.Timedelta(days=1)].copy()\n",
    "    test_df  = df.loc[start_date:end_date].copy()\n",
    "\n",
    "    # Skip if insufficient data (need enough for training and meaningful testing)\n",
    "    if len(train_df) < 200 or len(test_df) < 50:\n",
    "        print(f\"Skipping {regime_name} (insufficient data)\")\n",
    "        continue\n",
    "\n",
    "    # ========================================================================\n",
    "    # FEATURE SCALING\n",
    "    # ========================================================================\n",
    "    # CRITICAL: Fit scaler ONLY on training data to prevent data leakage\n",
    "    # Then transform both train and test using the same scaler parameters\n",
    "    scaler = MinMaxScaler()\n",
    "    scaler.fit(train_df[feature_cols])  # Learn min/max from training data only\n",
    "    \n",
    "    # Scale features to [0, 1] range for neural network input\n",
    "    train_scaled = scaler.transform(train_df[feature_cols])\n",
    "    test_scaled  = scaler.transform(test_df[feature_cols])\n",
    "\n",
    "    # Extract target values (log returns - already stationary, no scaling needed)\n",
    "    train_returns = train_df['LogReturn'].values\n",
    "    test_returns = test_df['LogReturn'].values\n",
    "\n",
    "    # ========================================================================\n",
    "    # HYPERPARAMETER GRID SEARCH\n",
    "    # ========================================================================\n",
    "    # Test multiple model configurations to find best hyperparameters\n",
    "    # window_size: How many past time steps the model looks at\n",
    "    # units: Number of GRU units (controls model capacity)\n",
    "    for window_size in [30, 50, 100]:\n",
    "        for units in [64, 128, 256]:\n",
    "            # Create sliding windows for sequence data\n",
    "            X_train, y_train = create_windows(train_scaled, train_returns, window_size)\n",
    "            X_test, y_test = create_windows(test_scaled, test_returns, window_size)\n",
    "\n",
    "            # Skip if no valid windows created\n",
    "            if len(X_train) == 0 or len(X_test) == 0:\n",
    "                continue\n",
    "\n",
    "            # ====================================================================\n",
    "            # MODEL ARCHITECTURE\n",
    "            # ====================================================================\n",
    "            # Build GRU (Gated Recurrent Unit) model for time series prediction\n",
    "            # Architecture: Two GRU layers (first returns sequences, second returns single value)\n",
    "            #               Followed by dense layers for final prediction\n",
    "            model = Sequential([\n",
    "                GRU(units, return_sequences=True),  # First GRU layer returns full sequence\n",
    "                GRU(units//2),                       # Second GRU layer reduces to single value\n",
    "                Dense(32, activation='relu'),        # Dense layer for non-linear transformation\n",
    "                Dense(1)                             # Output layer: predicts next log return\n",
    "            ])\n",
    "            model.compile(optimizer='adam', loss='mse')  # Mean Squared Error for regression\n",
    "\n",
    "            # Train model (verbose=0 suppresses training output)\n",
    "            history = model.fit(X_train, y_train,\n",
    "                                epochs=10, batch_size=32,\n",
    "                                verbose=0)\n",
    "\n",
    "            # ====================================================================\n",
    "            # PREDICTIONS AND TRADING SIGNAL GENERATION\n",
    "            # ====================================================================\n",
    "            # Get model predictions (log returns) for test period\n",
    "            real_preds = model.predict(X_test, verbose=0).flatten()\n",
    "            real_actual = y_test  # Actual log returns for comparison\n",
    "\n",
    "            if len(real_preds) < 2:\n",
    "                continue\n",
    "\n",
    "            # Generate trading signals based on predicted direction change\n",
    "            # Strategy: Compare consecutive predictions to determine if trend is strengthening\n",
    "            pred = real_preds[:-1]        # Current prediction\n",
    "            next_pred = real_preds[1:]    # Next period's prediction\n",
    "\n",
    "            # Position logic:\n",
    "            # +1 = Long position (buy): if next prediction > current prediction (expecting rise)\n",
    "            # -1 = Short position (sell): if next prediction < current prediction (expecting fall)\n",
    "            #  0 = Flat position (no trade): if predictions are equal\n",
    "            positions = np.zeros_like(pred)\n",
    "            positions[next_pred > pred] = 1   # Go long if model expects higher returns\n",
    "            positions[next_pred < pred] = -1  # Go short if model expects lower returns\n",
    "\n",
    "            # ====================================================================\n",
    "            # TRADING SIMULATION\n",
    "            # ====================================================================\n",
    "            # Align actual close prices with prediction horizon\n",
    "            # Skip first 'window_size' prices (needed for initial window)\n",
    "            actual_prices = test_df['Close'].values[int(window_size):]\n",
    "            if len(actual_prices) < 2:\n",
    "                continue\n",
    "            entry = actual_prices[:-1]  # Entry price for each trade\n",
    "            exit_ = actual_prices[1:]    # Exit price for each trade (next period)\n",
    "\n",
    "            # Initialize trading simulation\n",
    "            equity = 100_000  # Starting capital\n",
    "            equity_curve = [equity]  # Track equity over time\n",
    "            pnl_list = []  # Track profit/loss for each trade\n",
    "\n",
    "            # Simulate trading based on generated positions\n",
    "            for i in range(len(positions)):\n",
    "                pos = positions[i]  # Current position: -1 (short), 0 (flat), or +1 (long)\n",
    "\n",
    "                if pos == 0:\n",
    "                    # No position – carry equity forward without change\n",
    "                    equity_curve.append(equity)\n",
    "                    continue\n",
    "\n",
    "                # Risk management: risk 1% of current equity per trade\n",
    "                trade_size = equity * 0.01\n",
    "\n",
    "                if pos == 1:\n",
    "                    # Long position: profit if price rises from entry to exit\n",
    "                    pct_move = (exit_[i] - entry[i]) / entry[i]\n",
    "                else:  # pos == -1\n",
    "                    # Short position: profit if price falls from entry to exit\n",
    "                    pct_move = (entry[i] - exit_[i]) / entry[i]\n",
    "\n",
    "                # Calculate profit/loss for this trade\n",
    "                trade_pnl = trade_size * pct_move\n",
    "                equity += trade_pnl  # Update equity\n",
    "                pnl_list.append(trade_pnl)  # Record trade P&L\n",
    "                equity_curve.append(equity)  # Record equity after trade\n",
    "\n",
    "            # ====================================================================\n",
    "            # PERFORMANCE METRICS\n",
    "            # ====================================================================\n",
    "            # Calculate maximum drawdown: largest peak-to-trough decline in equity\n",
    "            equity_curve = np.array(equity_curve)\n",
    "            peaks = np.maximum.accumulate(equity_curve)  # Running maximum (peak equity)\n",
    "            drawdowns = peaks - equity_curve  # Absolute drawdowns from peak\n",
    "            drawdown_pcts = drawdowns / peaks  # Percentage drawdowns relative to each peak\n",
    "            max_drawdown = np.max(drawdowns)  # Maximum drawdown in absolute terms\n",
    "            max_drawdown_pct = np.max(drawdown_pcts)  # Maximum drawdown as percentage\n",
    "            \n",
    "            drawdown_pct = max_drawdown_pct  # Use percentage for results\n",
    "\n",
    "            # Calculate profit factor: ratio of gross profit to gross loss\n",
    "            # Profit Factor > 1 means strategy is profitable\n",
    "            gross_profit = sum(p for p in pnl_list if p > 0)  # Sum of all winning trades\n",
    "            gross_loss = -sum(p for p in pnl_list if p < 0)   # Sum of all losing trades (made positive)\n",
    "\n",
    "            # Skip configurations with no losses (would result in infinite profit factor)\n",
    "            if gross_loss == 0:\n",
    "                print(f\"Skipping W={window_size}, U={units} due to inf PF (no losing trades).\")\n",
    "                continue\n",
    "\n",
    "            profit_factor = gross_profit / gross_loss  # Key performance metric\n",
    "\n",
    "            # Count number of trades executed\n",
    "            num_trades = len(pnl_list)\n",
    "\n",
    "            # Filter out configurations with too few trades (likely overfitting or poor signal quality)\n",
    "            # Require at least 25% of trading days to have active positions\n",
    "            num_trading_days = len(positions)\n",
    "            if num_trades < 0.25 * num_trading_days:\n",
    "                print(f\"Skipping W={window_size}, U={units} due to insufficient trades ({num_trades} < {0.25 * num_trading_days:.0f}).\")\n",
    "                continue\n",
    "            \n",
    "            # ====================================================================\n",
    "            # RETURN ANALYSIS\n",
    "            # ====================================================================\n",
    "            # Calculate strategy returns: multiply position by actual next-period return\n",
    "            # Long positions (+1) profit from positive returns, short positions (-1) profit from negative returns\n",
    "            model_trade_logrets = []\n",
    "            for i in range(len(positions)):\n",
    "                pos = positions[i]\n",
    "                if pos != 0 and i + 1 < len(real_actual):\n",
    "                    # Strategy log return = position direction * actual next-day log return\n",
    "                    model_trade_logrets.append(pos * real_actual[i+1])\n",
    "                else:\n",
    "                    model_trade_logrets.append(0.0)  # No position = zero return\n",
    "\n",
    "            model_trade_logrets = np.array(model_trade_logrets)\n",
    "\n",
    "            # Calculate cumulative returns for strategy and buy-and-hold\n",
    "            model_cum_log_ret = np.cumsum(model_trade_logrets)  # Strategy cumulative returns\n",
    "            asset_cum_log_ret = np.cumsum(real_actual)          # Buy & hold cumulative returns\n",
    "\n",
    "            # ====================================================================\n",
    "            # DIRECTIONAL ACCURACY\n",
    "            # ====================================================================\n",
    "            # Measure how often the model correctly predicts price direction\n",
    "            # Only evaluated when model has an active position (not flat)\n",
    "            positions_arr = np.array(positions)\n",
    "            valid_mask = positions_arr != 0  # Only consider periods with active positions\n",
    "\n",
    "            if valid_mask.sum() > 0:\n",
    "                # Compare model's predicted direction (from position) with actual price direction\n",
    "                asset_dir = np.sign(real_actual[1:len(positions_arr)+1])  # Actual direction\n",
    "                model_dir = positions_arr[valid_mask]                      # Model's direction\n",
    "                asset_dir_valid = asset_dir[valid_mask]\n",
    "                directional_accuracy = np.mean(model_dir == asset_dir_valid)  # Percentage correct\n",
    "            else:\n",
    "                directional_accuracy = np.nan  # No positions to evaluate\n",
    "\n",
    "            print(f\"Directional Accuracy: {directional_accuracy:.2%}\")\n",
    "\n",
    "            # ====================================================================\n",
    "            # PREDICTION ERROR METRICS\n",
    "            # ====================================================================\n",
    "            # Measure how well model predicts actual log returns (regression metrics)\n",
    "            rmse = np.sqrt(mean_squared_error(real_actual, real_preds))  # Root Mean Squared Error\n",
    "            mae = mean_absolute_error(real_actual, real_preds)           # Mean Absolute Error\n",
    "            r2 = r2_score(real_actual, real_preds)                        # R-squared (coefficient of determination)\n",
    "\n",
    "            print(f\"{regime_name} | W:{window_size} U:{units} | PF:{profit_factor:.2f} | DD:{drawdown_pct:.2%}\")\n",
    "            \n",
    "            # Print error metrics\n",
    "            print(f\"RMSE: {rmse:.4f} | MAE: {mae:.4f} | R2: {r2:.4f}\")\n",
    "            \n",
    "            # Print numbers of trades  \n",
    "            print(f\"Number of trades executed: {num_trades}\")\n",
    "            \n",
    "            # ====================================================================\n",
    "            # VISUALIZATION\n",
    "            # ====================================================================\n",
    "            # Plot cumulative returns comparison: strategy vs buy-and-hold\n",
    "            plt.style.use('default')\n",
    "            plt.figure(figsize=(12,5))\n",
    "            plt.plot(model_cum_log_ret, label=f\"GRU Strategy (W={window_size}, U={units})\")\n",
    "            plt.plot(asset_cum_log_ret, label=\"Buy & Hold\")\n",
    "            plt.title(f\"Cumulative Log Returns — {regime_name} (W={window_size}, U={units})\")\n",
    "            plt.xlabel(\"Time Steps\")\n",
    "            plt.ylabel(\"Cumulative Log Return\")\n",
    "            plt.legend()\n",
    "            plt.grid(True, alpha=0.3)\n",
    "            # Save plot with sanitized filename\n",
    "            plot_name = f\"{regime_name.replace('/','_').replace(' ','_')}_W{window_size}_U{units}_cumlogret.svg\"\n",
    "            plt.savefig(plot_name, format='svg')\n",
    "            plt.show()\n",
    "            \n",
    "            # ====================================================================\n",
    "            # STORE RESULTS\n",
    "            # ====================================================================\n",
    "            # Save all metrics for this configuration for later analysis\n",
    "            regime_results.append({\n",
    "                \"Regime\": regime_name,\n",
    "                \"Start\": start_date,\n",
    "                \"End\": end_date,\n",
    "                \"Window\": window_size,\n",
    "                \"Units\": units,\n",
    "                \"ProfitFactor\": profit_factor,\n",
    "                \"MaxDrawdown\": drawdown_pct,\n",
    "                \"RMSE\": rmse,\n",
    "                \"MAE\": mae,\n",
    "                \"R2\": r2,\n",
    "                \"DirectionalAccuracy\": directional_accuracy,\n",
    "                \"ModelCumLogRetEnd\": float(model_cum_log_ret[-1]),\n",
    "                \"AssetCumLogRetEnd\": float(asset_cum_log_ret[-1])\n",
    "            })\n",
    "\n",
    "# ============================================================================\n",
    "# RESULTS SUMMARY\n",
    "# ============================================================================\n",
    "# Convert results to DataFrame and sort by regime, then by profit factor (descending)\n",
    "# This allows easy identification of best performing models per regime\n",
    "results_df = pd.DataFrame(regime_results).sort_values([\"Regime\", \"ProfitFactor\"], ascending=[True, False])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d3f16d6",
   "metadata": {},
   "source": [
    "<a id=\"monte-carlo-permutation-test\"></a>\n",
    "*Monte Carlo Permutation Test*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc7aa704",
   "metadata": {},
   "outputs": [],
   "source": [
    "def monte_carlo_permutation_pf(X_train, y_train, X_test, y_test, units, actual_prices,\n",
    "                                num_permutations=50, epochs=3, use_simple_model=True):\n",
    "    \"\"\"\n",
    "    Monte Carlo Permutation Test for statistical significance of trading strategy.\n",
    "    \n",
    "    This function tests whether the model's profit factor is statistically significant\n",
    "    by comparing it against a null distribution where labels are randomly shuffled.\n",
    "    If the real profit factor is significantly higher than random permutations,\n",
    "    it suggests the model has genuine predictive power rather than just luck.\n",
    "    \n",
    "    Args:\n",
    "        X_train: Training feature windows\n",
    "        y_train: Training target values (log returns)\n",
    "        X_test: Test feature windows\n",
    "        y_test: Test target values (log returns)\n",
    "        units: Number of GRU units in the model\n",
    "        actual_prices: np.array of actual closing prices aligned with y_test (for P&L calculation)\n",
    "        num_permutations: Number of random permutations to run (default: 50)\n",
    "        epochs: Number of training epochs for each permutation (default: 3, reduced for speed)\n",
    "        use_simple_model: If True, use smaller model (64 units max) for faster computation\n",
    "    \n",
    "    Returns:\n",
    "        pf_list: List of profit factors from all permutations (null distribution)\n",
    "    \"\"\"\n",
    "    from tensorflow.keras.callbacks import EarlyStopping\n",
    "    \n",
    "    pf_list = []  # Store profit factors from all permutations\n",
    "    \n",
    "    # Determine model complexity: use simpler model for speed if flag is set\n",
    "    if use_simple_model:\n",
    "        model_units = min(units, 64)  # Cap at 64 units for faster computation\n",
    "    else:\n",
    "        model_units = units  # Use full model complexity\n",
    "    \n",
    "    # Early stopping to prevent overfitting during permutation tests\n",
    "    early_stop = EarlyStopping(monitor='loss', patience=2, restore_best_weights=False, verbose=0)\n",
    "    \n",
    "    print(f\"Running {num_permutations} permutations (epochs={epochs}, units={model_units})...\", end=\"\", flush=True)\n",
    "\n",
    "    # Run multiple permutations to build null distribution\n",
    "    for perm_idx in range(num_permutations):\n",
    "        if (perm_idx + 1) % 10 == 0:\n",
    "            print(f\" {perm_idx + 1}/{num_permutations}\", end=\"\", flush=True)\n",
    "\n",
    "        # Step 1: Shuffle training labels to break any real relationships\n",
    "        # This creates a null hypothesis where there's no predictive signal\n",
    "        y_perm = np.random.permutation(y_train)\n",
    "\n",
    "        # Step 2: Train model on permuted (randomized) data\n",
    "        # Same architecture as real model, but trained on meaningless labels\n",
    "        perm_model = Sequential([\n",
    "                GRU(model_units, return_sequences=True),\n",
    "                GRU(model_units//2),\n",
    "                Dense(32, activation='relu'),\n",
    "                Dense(1)\n",
    "            ])\n",
    "        perm_model.compile(optimizer='adam', loss='mse')\n",
    "        perm_model.fit(X_train, y_perm, epochs=epochs, batch_size=32, \n",
    "                      callbacks=[early_stop], verbose=0)\n",
    "\n",
    "        # Step 3: Generate predictions with permuted model\n",
    "        perm_preds = perm_model.predict(X_test, verbose=0, batch_size=128).flatten()\n",
    "\n",
    "        # Step 4: Apply same trading logic as real model\n",
    "        # This tests if random predictions can generate similar profit factors\n",
    "        entry = actual_prices[:-1]\n",
    "        exit_ = actual_prices[1:]\n",
    "        pred = perm_preds[:-1]\n",
    "        next_pred = perm_preds[1:]\n",
    "\n",
    "        # Generate positions using same logic as real model\n",
    "        positions = np.zeros_like(pred)\n",
    "        positions[next_pred > pred] = 1   # Long if expecting increase\n",
    "        positions[next_pred < pred] = -1 # Short if expecting decrease\n",
    "\n",
    "        # Simulate trading with same parameters as real model\n",
    "        equity = 100_000\n",
    "        pnl_list = []\n",
    "\n",
    "        for i in range(len(positions)):\n",
    "            pos = positions[i]\n",
    "            if pos == 0:\n",
    "                continue\n",
    "\n",
    "            trade_size = equity * 0.01  # Same 1% risk per trade\n",
    "\n",
    "            if pos == 1:\n",
    "                # Long position\n",
    "                pct_move = (exit_[i] - entry[i]) / entry[i]\n",
    "            else:  # pos == -1\n",
    "                # Short position\n",
    "                pct_move = (entry[i] - exit_[i]) / entry[i]\n",
    "\n",
    "            trade_pnl = trade_size * pct_move\n",
    "            equity += trade_pnl\n",
    "            pnl_list.append(trade_pnl)\n",
    "\n",
    "        # Calculate profit factor for this permutation\n",
    "        gross_profit = sum(p for p in pnl_list if p > 0)\n",
    "        gross_loss = -sum(p for p in pnl_list if p < 0)\n",
    "        pf = gross_profit / gross_loss if gross_loss > 0 else np.inf\n",
    "        pf_list.append(pf)\n",
    "        \n",
    "        # Clean up model to free memory (important for many iterations)\n",
    "        del perm_model\n",
    "        tf.keras.backend.clear_session()\n",
    "\n",
    "    print(\" Done!\")\n",
    "    return pf_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bf76b0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# VISUALIZATION FUNCTION\n",
    "# ============================================================================\n",
    "def plot_mcpt_histogram(pf_distribution, real_pf, regime_name, window, units, p_value, mean_pf):\n",
    "    \"\"\"\n",
    "    Plot histogram of Monte Carlo Permutation Test results.\n",
    "    \n",
    "    Visualizes the null distribution (from random permutations) and compares it\n",
    "    to the real profit factor. If real PF is far to the right of the distribution,\n",
    "    it suggests statistical significance.\n",
    "    \n",
    "    Args:\n",
    "        pf_distribution: List of profit factors from permutations (null distribution)\n",
    "        real_pf: Actual profit factor from real model\n",
    "        regime_name: Name of market regime being tested\n",
    "        window: Window size used in model\n",
    "        units: Number of GRU units used in model\n",
    "        p_value: Statistical p-value (proportion of permutations >= real_pf)\n",
    "        mean_pf: Mean profit factor of null distribution\n",
    "    \"\"\"\n",
    "    plt.style.use('default')\n",
    "    pd.Series(pf_distribution).hist(color='blue', label='Permutations', bins=30)\n",
    "    plt.axvline(real_pf, color='red', linestyle='--', linewidth=2, label=f'Real PF: {real_pf:.2f}')\n",
    "    plt.axvline(mean_pf, color='black', linestyle='--', linewidth=2, label=f'Mean PF: {mean_pf:.2f}')\n",
    "    plt.xlabel(\"Profit Factor\", fontsize=12)\n",
    "    plt.ylabel(\"Frequency\", fontsize=12)\n",
    "    plt.title(f\"MCPT: {regime_name} | W={window}, U={units} | P-Value: {p_value:.4f}\", fontsize=13)\n",
    "    plt.grid(alpha=0.3)\n",
    "    plt.legend(fontsize=11)\n",
    "    plt.tight_layout()\n",
    "    plot_name = f\"{regime_name.replace('/','_').replace(' ','_')}_W{window}_U{units}_mcpt.svg\"\n",
    "    plt.savefig(plot_name, format='svg')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d37a8c99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Monte Carlo Permutation Test Configuration ===\n",
    "# Set to True to enable Monte Carlo testing (OPTIMIZED)\n",
    "RUN_MONTE_CARLO = True  # Set to True to enable\n",
    "\n",
    "# Options for Monte Carlo testing:\n",
    "# - \"all\": Run on all model configurations (slower)\n",
    "# - \"best_per_regime\": Run only on best model per regime (recommended)\n",
    "# - \"best_overall\": Run only on overall best model (fastest)\n",
    "MCPT_MODE = \"best_per_regime\"  # Options: \"all\", \"best_per_regime\", \"best_overall\"\n",
    "\n",
    "# Monte Carlo test parameters (optimized for speed)\n",
    "MCPT_NUM_PERMUTATIONS = 30  # 30-50 is usually sufficient\n",
    "MCPT_EPOCHS = 3  # 3-5 is usually sufficient\n",
    "MCPT_USE_SIMPLE_MODEL = False  # Use simpler model for permutation tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bcc19a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Run Monte Carlo Permutation Test on Best Models (OPTIONAL) ===\n",
    "# This runs the optimized MCPT only on the best models, saving significant time\n",
    "\n",
    "if RUN_MONTE_CARLO and MCPT_MODE in [\"best_per_regime\", \"best_overall\"]:\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"Running OPTIMIZED Monte Carlo Permutation Tests\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    if MCPT_MODE == \"best_per_regime\":\n",
    "        # Run MCPT on the best performing model for each regime separately\n",
    "        # This tests statistical significance for each market condition independently\n",
    "        for regime_name in regimes.keys():\n",
    "            regime_best = results_df[results_df[\"Regime\"] == regime_name].iloc[0]\n",
    "            print(f\"\\n--- Testing best model for {regime_name} ---\")\n",
    "            print(f\"Config: W={regime_best['Window']}, U={regime_best['Units']}, PF={regime_best['ProfitFactor']:.2f}\")\n",
    "            \n",
    "            # Reconstruct the same data split and preprocessing used during training\n",
    "            start_date, end_date = regimes[regime_name]\n",
    "            train_df = df.loc[:pd.to_datetime(start_date) - pd.Timedelta(days=1)].copy()\n",
    "            test_df = df.loc[start_date:end_date].copy()\n",
    "            \n",
    "            # Reapply same scaling (fit on train, transform both)\n",
    "            scaler = MinMaxScaler()\n",
    "            scaler.fit(train_df[feature_cols])\n",
    "            train_scaled = scaler.transform(train_df[feature_cols])\n",
    "            test_scaled = scaler.transform(test_df[feature_cols])\n",
    "            \n",
    "            # Extract returns and get best model's hyperparameters\n",
    "            train_returns = train_df['LogReturn'].values\n",
    "            test_returns = test_df['LogReturn'].values\n",
    "            window = int(regime_best['Window'])  # Best window size for this regime\n",
    "            units = int(regime_best['Units'])     # Best number of units for this regime\n",
    "            \n",
    "            # Create windows using best model's configuration\n",
    "            X_train, y_train = create_windows(train_scaled, train_returns, window)\n",
    "            X_test, y_test = create_windows(test_scaled, test_returns, window)\n",
    "            actual_prices = test_df['Close'].values[window:]  # Prices aligned with predictions\n",
    "            \n",
    "            if len(X_test) == 0 or len(actual_prices) < 2:\n",
    "                print(\"Skipping MCPT (insufficient samples)\")\n",
    "                continue\n",
    "            \n",
    "            pf_distribution = monte_carlo_permutation_pf(\n",
    "                X_train, y_train, X_test, y_test,\n",
    "                units,\n",
    "                actual_prices,\n",
    "                num_permutations=MCPT_NUM_PERMUTATIONS,\n",
    "                epochs=MCPT_EPOCHS,\n",
    "                use_simple_model=MCPT_USE_SIMPLE_MODEL\n",
    "            )\n",
    "            \n",
    "            # Calculate p-value: proportion of permutations with PF >= real PF\n",
    "            # Low p-value (< 0.05) means real PF is unlikely under null hypothesis (statistically significant)\n",
    "            p_value = np.mean(np.array(pf_distribution) >= regime_best['ProfitFactor'])\n",
    "            print(f\"  MCPT Results: p-value = {p_value:.4f}\")\n",
    "            print(f\"  Random PF distribution: {np.mean(pf_distribution):.2f} ± {np.std(pf_distribution):.2f}\")\n",
    "            print(f\"  {' Statistically significant' if p_value < 0.05 else ' Not statistically significant'} (p < 0.05)\")\n",
    "            \n",
    "            mean_pf = np.mean(pf_distribution)\n",
    "            plot_mcpt_histogram(\n",
    "                pf_distribution,\n",
    "                regime_best['ProfitFactor'],\n",
    "                regime_name,\n",
    "                window,\n",
    "                units,\n",
    "                p_value,\n",
    "                mean_pf  # Added parameter for mean PF\n",
    "            )\n",
    "    \n",
    "    elif MCPT_MODE == \"best_overall\":\n",
    "        # Run MCPT on the single best model across all regimes\n",
    "        # This is faster but less comprehensive than testing each regime separately\n",
    "        best_overall = results_df.iloc[0]  # First row is best (sorted by ProfitFactor descending)\n",
    "        print(f\"\\n--- Testing overall best model ---\")\n",
    "        print(f\"Regime: {best_overall['Regime']}\")\n",
    "        print(f\"Config: W={best_overall['Window']}, U={best_overall['Units']}, PF={best_overall['ProfitFactor']:.2f}\")\n",
    "        \n",
    "        start_date, end_date = regimes[best_overall['Regime']]\n",
    "        train_df = df.loc[:pd.to_datetime(start_date) - pd.Timedelta(days=1)].copy()\n",
    "        test_df = df.loc[start_date:end_date].copy()\n",
    "        \n",
    "        scaler = MinMaxScaler()\n",
    "        scaler.fit(train_df[feature_cols])\n",
    "        train_scaled = scaler.transform(train_df[feature_cols])\n",
    "        test_scaled = scaler.transform(test_df[feature_cols])\n",
    "        \n",
    "        train_returns = train_df['LogReturn'].values\n",
    "        test_returns = test_df['LogReturn'].values\n",
    "        window = int(best_overall['Window'])\n",
    "        units = int(best_overall['Units'])\n",
    "        \n",
    "        X_train, y_train = create_windows(train_scaled, train_returns, window)\n",
    "        X_test, y_test = create_windows(test_scaled, test_returns, window)\n",
    "        actual_prices = test_df['Close'].values[window:]\n",
    "        \n",
    "        if len(X_test) == 0 or len(actual_prices) < 2:\n",
    "            print(\"Skipping MCPT (insufficient samples)\")\n",
    "        else:\n",
    "            pf_distribution = monte_carlo_permutation_pf(\n",
    "                X_train, y_train, X_test, y_test,\n",
    "                units,\n",
    "                actual_prices,\n",
    "                num_permutations=MCPT_NUM_PERMUTATIONS,\n",
    "                epochs=MCPT_EPOCHS,\n",
    "                use_simple_model=MCPT_USE_SIMPLE_MODEL\n",
    "            )\n",
    "            \n",
    "            p_value = np.mean(np.array(pf_distribution) >= best_overall['ProfitFactor'])\n",
    "            print(f\"  MCPT Results: p-value = {p_value:.4f}\")\n",
    "            print(f\"  Random PF distribution: {np.mean(pf_distribution):.2f} ± {np.std(pf_distribution):.2f}\")\n",
    "            print(f\"  {' Statistically significant' if p_value < 0.05 else ' Not statistically significant'} (p < 0.05)\")\n",
    "            \n",
    "            mean_pf = np.mean(pf_distribution)\n",
    "            plot_mcpt_histogram(\n",
    "                pf_distribution,\n",
    "                best_overall['ProfitFactor'],\n",
    "                best_overall['Regime'],\n",
    "                window,\n",
    "                units,\n",
    "                p_value,\n",
    "                mean_pf  \n",
    "            )\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "else:\n",
    "    print(\"\\n Tip: Set RUN_MONTE_CARLO=True to enable Monte Carlo permutation testing\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
